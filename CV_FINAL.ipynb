{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### %%writefile test\n",
    "## Final Exam\n",
    "<b>The general objective is to solve five different games.<br>\n",
    "The code for each game should be written in an individual cell.<br>\n",
    "Add comments in your code to explain your approach.<br>\n",
    "You can design your own approach, and use any method you learned in this class.</b><br>\n",
    "\n",
    "1A. Input images from video file WiiPlay.mp4 with level 15 (frame number between 4820 and 5000).<br> \n",
    "1B. (5pts) Acquire a <b>face template</b> from the first frame (frame number = 4820).<br>\n",
    "1C. (10pts) Try to detect the face the same as the template on subsequent frames, draw a <b>red</b> rectangle around the detected face, and show the output images in the <b>\"find_this_mii\"</b> window.<br><br>\n",
    "\n",
    "2A. Input images from video file WiiPlay.mp4 with level 8 (frame number between 2180 and 2380).<br>\n",
    "2B. (5pts) Detect <b>pedestrians</b> on each frame and draw a <b>green</b> rectangle around your detection.<br>\n",
    "2C. (5pts) Detect <b>faces</b> on each frame and draw a <b>blue</b> rectangle around your detection.<br>\n",
    "2D. (10pts) Try to find two faces look alike each other, draw a <b>red</b> rectangle around each of the two faces, and show the output images in the <b>\"find_two_look_alike\"</b> window.<br><br>\n",
    "\n",
    "3A. Input images from video file WiiPlay.mp4 with level 9 (frame number between 2480 and 2600).<br>\n",
    "3B. (5pts) <b>Detect </b>faces(or pedestrians) on the first frame and draw a <b>blue</b> rectangle around your detection.<br>\n",
    "3C. (10pts) <b>Track </b>faces(or pedestrians) on subsequent frames and draw a <b>green</b> rectangle around your tracking.<br>\n",
    "3D. (5pts) Try to find out the fastest character, draw a <b>red</b> rectangle around the fastest character, and show the output images in the <b>\"find_the_fastest_character\"</b> window.<br><br>\n",
    "\n",
    "4A. Input images from video file WiiPlay.mp4 with level 6 (frame number between 1650 and 1800).<br>\n",
    "4B. (10pts) Compute and show <b>optical flows</b> on each frame using <b>blue</b> arrows.<br>\n",
    "4C. (5pts) Try to detect two odd character who face the opposite direction from everyone else, draw a <b>red</b> rectangle around each of the two character, and show the output images in the <b>\"find_two_odds\"</b> window.<br><br>\n",
    "\n",
    "5A. Input continuous BGR images from webcam.\n",
    "5B. (5pts) Use <i>MediaPipe()</i> to detect and track one of your hands\n",
    "5C. (5pts) Obtain the positions of 21 HandLandmarks, draw a <b>blue</b> circle around each HandLandmark.\n",
    "5D. (10pts) Design an algorithm to recognize three hand gestures of Rock, Scissor, Paper. Write the type of the recognized hand gesture on the upper left corner using <i>cv2.putText()</i>.\n",
    "\n",
    "6. (5pts) Any comments regarding the final exam? Which steps you believe you have completed? Which steps bother you?<br>\n",
    "7. (5pts) Any suggestion to teaching assistants to improve this class? Any suggestion to teacher to improve this class?<br>\n",
    "8. Upload your Jupyter file (*.ipynb) with code and report . "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# problem 1\n",
    "# frames 4820 - 5000 ( lvl15 )\n",
    "# from first frame detect mii face then locate it after\n",
    "\n",
    "from typing import Iterator, Union, Tuple, List\n",
    "import numpy, cv2\n",
    "\n",
    "class VideoReader:\n",
    "    \n",
    "    video_stream  = None\n",
    "    start_frame   = None\n",
    "    num_frames    = None\n",
    "    frame_ratio   = None\n",
    "    frame_counter = None\n",
    "    \n",
    "    @classmethod\n",
    "    def initialize(class_, video_file_name : str) -> \"VideoReader\":\n",
    "        class_.video_stream = cv2.VideoCapture(video_file_name)\n",
    "        if not (class_.video_stream.isOpened()):\n",
    "            raise IOError(\"Cannot open video file \\\"{}\\\"\\n\".format(video_file_name))\n",
    "        return class_\n",
    "    \n",
    "    @classmethod\n",
    "    def configure(class_, start_frame : int, num_frames : int, frame_ratio : float = 1) -> \"VideoReader\":\n",
    "        (class_.start_frame, class_.num_frames, class_.frame_ratio, class_.frame_counter) = (\n",
    "            start_frame, num_frames, frame_ratio, num_frames + 1\n",
    "        )\n",
    "        return class_\n",
    "    \n",
    "    @classmethod\n",
    "    def read(class_) -> Iterator[ numpy.ndarray ]:\n",
    "        ratio = class_.frame_ratio\n",
    "        while True:\n",
    "            if (class_.num_frames < class_.frame_counter):\n",
    "                class_.frame_counter = 0\n",
    "                class_.video_stream.set(cv2.CAP_PROP_POS_FRAMES, class_.start_frame)\n",
    "            latest_frame = class_.video_stream.read()[1]\n",
    "            yield ((latest_frame) if (ratio == 1) else (cv2.resize(latest_frame, None, None, fx = ratio, fy = ratio)))\n",
    "            class_.frame_counter += 1\n",
    "            \n",
    "    @classmethod\n",
    "    def set_frame(class_, frame_number : int) -> \"VideoReader\":\n",
    "        \n",
    "        class_.video_stream.set(cv2.CAP_PROP_POS_FRAMES, frame_number)\n",
    "        \n",
    "        return class_\n",
    "    \n",
    "    @classmethod\n",
    "    def reset_counter(class_):\n",
    "        class_.frame_counter = class_.num_frames + 1\n",
    "            \n",
    "    @classmethod\n",
    "    def release(class_):\n",
    "        \n",
    "        class_.video_stream.release()\n",
    "        class_.video_stream  = None\n",
    "        class_.start_frame   = None\n",
    "        class_.num_frames    = None\n",
    "        class_.frame_ratio   = None\n",
    "        class_.frame_counter = None\n",
    "\n",
    "class BasicFaceDetector:\n",
    "    \n",
    "    face_detector = None\n",
    "    detector_file_name = \"./haarcascade_frontalface_alt2.xml\"\n",
    "    \n",
    "    @classmethod\n",
    "    def initialize(class_) -> \"BasicFaceDetector\":\n",
    "        class_.face_detector = cv2.CascadeClassifier(class_.detector_file_name)\n",
    "        if (class_.face_detector.empty()):\n",
    "            raise IOError(\"Cannot find XML file \\\"{}\\\"\\n\".format(class_.detector_file_name))\n",
    "        return class_\n",
    "    \n",
    "    @classmethod\n",
    "    def detect(class_, source_image : numpy.ndarray, ** kwargs) -> Union[ numpy.ndarray, tuple ]:\n",
    "        return class_.face_detector.detectMultiScale(source_image, ** kwargs)\n",
    "    \n",
    "class ImageUtils:\n",
    "    \n",
    "    @staticmethod\n",
    "    def draw_boundary_boxes(display_frame : numpy.ndarray, \n",
    "            boundary_boxes : Union[ type(None), List[ tuple ]], color = (0, 0, 255), width = 5) -> numpy.ndarray:\n",
    "        \n",
    "        if (boundary_boxes is None):\n",
    "            return display_frame\n",
    "        \n",
    "        new_frame = numpy.copy(display_frame)\n",
    "        \n",
    "        for (x, y, w, h) in boundary_boxes:\n",
    "            new_frame = cv2.rectangle(new_frame, (x, y), (x + w, y + h), color, width)\n",
    "            \n",
    "        return new_frame\n",
    "    \n",
    "    @staticmethod\n",
    "    def enlarge_boundary_boxes(\n",
    "            boundary_boxes : List[ tuple ], ratio : float, max_height : int, max_width : int) -> List[ tuple ]:\n",
    "        \n",
    "        def enlarge(boundary_box : tuple) -> tuple:\n",
    "            (x, y, w, h) = boundary_box\n",
    "            (new_w, new_h) = (int(w * ratio), int(h * ratio))\n",
    "            (dif_w, dif_h) = (abs(w - new_w) // 2, abs(h - new_h) // 2)\n",
    "            (end_x, end_y) = (x + new_w, y + new_h)\n",
    "            return (\n",
    "                max(0, x - dif_w),\n",
    "                max(0, y - dif_h),\n",
    "                min(max_width, (end_x - x)),\n",
    "                min(max_height, (end_y - y))\n",
    "            )\n",
    "        return list(map(enlarge, boundary_boxes))\n",
    "\n",
    "if (__name__ == \"__main__\"):\n",
    "    \n",
    "    start_frame     = 4820\n",
    "    \n",
    "    num_frames      = 180\n",
    "    \n",
    "    frame_ratio     = 0.75\n",
    "    \n",
    "    threshold       = 0.96\n",
    "    \n",
    "    window_name     = \"find_this_mii\"\n",
    "    \n",
    "    video_file_name = \"./wiiplay.mp4\"\n",
    "    \n",
    "    # load the video file and initialize the frame range to loop\n",
    "    VideoReader.initialize(video_file_name).configure(start_frame, num_frames, frame_ratio)\n",
    "    \n",
    "    # initialize the haarcascade face detector \n",
    "    # must first download \"haarcascade_frontalface_alt2.xml\" from the official GitHub repository\n",
    "    BasicFaceDetector.initialize()\n",
    "    \n",
    "    # convert an image from BGR to HSV\n",
    "    def to_hsv(current_frame : numpy.ndarray) -> numpy.ndarray:\n",
    "        \n",
    "        return cv2.cvtColor(current_frame, cv2.COLOR_BGR2HSV)\n",
    "    \n",
    "    # resize an image by a scaling factor\n",
    "    def resize_face(face_image : numpy.ndarray, scale : float = 0.90) -> numpy.ndarray:\n",
    "        \n",
    "        return cv2.resize(face_image, None, None, fx = scale, fy = scale)\n",
    "    \n",
    "    # obtain a face template from the first frame\n",
    "    def fetch_template() -> numpy.ndarray:\n",
    "        \n",
    "        # fetch the first frame from the video\n",
    "        current_frame = next(VideoReader.read())\n",
    "        \n",
    "        # detect and obtain the bounding box of the face from the first frame\n",
    "        boundary_box = BasicFaceDetector.detect(\n",
    "            cv2.cvtColor(current_frame, cv2.COLOR_BGR2GRAY), scaleFactor = 1.2, minSize = (40, 40), maxSize = (95, 95))\n",
    "        \n",
    "        # raise an exception if no face is detected in the first frame\n",
    "        if (boundary_box.__len__() < 1):\n",
    "            \n",
    "            raise Exception(\"Cannot capture template face\\n\")\n",
    "            \n",
    "        # unpack values from the bounding box tuple\n",
    "        (x, y, w, h) = boundary_box[0]\n",
    "        \n",
    "        # crop the first frame according to the bounding box and return the cropped image\n",
    "        return current_frame[ y : y + h, x : x + w ]\n",
    "    \n",
    "    # obtain the bounding box of a connected region\n",
    "    def comp_to_boundary_box(c : numpy.ndarray, i : int) -> Tuple[ int ]:\n",
    "        \n",
    "        # find the coordinates of a connected region labeled i\n",
    "        (a, b) = numpy.where(c == i)\n",
    "        \n",
    "        # find the upper left corner of the connected region\n",
    "        (q, r) = (numpy.min(b), numpy.min(a))\n",
    "        \n",
    "        # find the lower right corner and return the bounding box as a tuple\n",
    "        return (int(q), int(r), int(numpy.max(b) - q), int(numpy.max(a) - r))\n",
    "    \n",
    "    # obtain the template image \n",
    "    template_face = resize_face(to_hsv(fetch_template()))\n",
    "    \n",
    "    # endlessly reading frames from video loop until Esc is pressed\n",
    "    for frame in VideoReader.read():\n",
    "        \n",
    "        # detect faces in the current frame and obtain their bounding boxes\n",
    "        faces        = BasicFaceDetector.detect(frame, scaleFactor = 1.2, minSize = (40, 40), maxSize = (95, 95))\n",
    "        \n",
    "        # perform template matching to find the template image in the current frame\n",
    "        match_result = cv2.matchTemplate(to_hsv(frame), template_face, cv2.TM_CCOEFF)\n",
    "        \n",
    "        # basic normalization\n",
    "        match_result = cv2.normalize(match_result, None, 0, 1, cv2.NORM_MINMAX)\n",
    "        \n",
    "        # filter out low confidence matching via thresholding, and locate the coordinates\n",
    "        match_result = numpy.where(match_result >= threshold)\n",
    "        \n",
    "        # convert the coordinates to standard bounding boxes\n",
    "        match_boundary_boxes = list( \n",
    "            (x, y, template_face.shape[1], template_face.shape[0]) for (x, y) in zip(match_result[1], match_result[0])  )\n",
    "        \n",
    "        # initialize an empty mask\n",
    "        match_mask  = numpy.zeros(shape = frame.shape, dtype = numpy.uint8)\n",
    "        \n",
    "        # initialize an empty mask\n",
    "        faces_mask  = numpy.zeros(shape = frame.shape, dtype = numpy.uint8)\n",
    "        \n",
    "        # mark the regions with matches by drawing solid rectangles\n",
    "        match_mask  = ImageUtils.draw_boundary_boxes(match_mask, match_boundary_boxes, (255, 255, 255), -1)\n",
    "        \n",
    "        # mark the regions with faces detected by drawing solid rectangles\n",
    "        faces_mask  = ImageUtils.draw_boundary_boxes(faces_mask, faces, (255, 255, 255), -1)\n",
    "            \n",
    "        # merge the two masks using \"bitwise_and\" to obtain regions of matched faces\n",
    "        result_mask = cv2.bitwise_and(match_mask, faces_mask)\n",
    "        \n",
    "        # reduce the mask from 3 channels to just 1 channel (grayscale)\n",
    "        result_mask = cv2.cvtColor(result_mask, cv2.COLOR_BGR2GRAY)\n",
    "        \n",
    "        # find the connected components in the merged mask\n",
    "        n, c        = cv2.connectedComponents(result_mask)\n",
    "        \n",
    "        # only consider cases with at least one component ( background (=1) + components (>=1) -> (>1) )\n",
    "        if (n > 1):\n",
    "            \n",
    "            # obtain bounding boxes from component regions and stack them together\n",
    "            comp  = numpy.stack([  comp_to_boundary_box(c, i) for i in range(1, n)  ])\n",
    "            \n",
    "            # draw red bounding boxes on faces that satisfy what the description demands\n",
    "            frame = ImageUtils.draw_boundary_boxes(frame, comp, (0, 0, 255), 4)\n",
    "                \n",
    "        # show the resulting image\n",
    "        cv2.imshow(window_name, frame)\n",
    "        \n",
    "        # terminate the program when Esc is pressed\n",
    "        if (cv2.waitKey(1) == 27):\n",
    "            \n",
    "            break\n",
    "            \n",
    "    cv2.destroyWindow(window_name)\n",
    "    \n",
    "    VideoReader.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# problem 2\n",
    "# frames 2180 - 2380 ( lvl8 )\n",
    "# detect pedestrians, their faces, and find two identicals\n",
    "\n",
    "from typing import Iterator, Union, Tuple, List\n",
    "from imutils.object_detection import non_max_suppression\n",
    "import face_recognition, itertools, mediapipe, numpy, cv2\n",
    "\n",
    "class VideoReader:\n",
    "    \n",
    "    video_stream  = None\n",
    "    start_frame   = None\n",
    "    num_frames    = None\n",
    "    frame_ratio   = None\n",
    "    frame_counter = None\n",
    "    \n",
    "    @classmethod\n",
    "    def initialize(class_, video_file_name : str) -> \"VideoReader\":\n",
    "        class_.video_stream = cv2.VideoCapture(video_file_name)\n",
    "        if not (class_.video_stream.isOpened()):\n",
    "            raise IOError(\"Cannot open video file \\\"{}\\\"\\n\".format(video_file_name))\n",
    "        return class_\n",
    "    \n",
    "    @classmethod\n",
    "    def configure(class_, start_frame : int, num_frames : int, frame_ratio : float = 1) -> \"VideoReader\":\n",
    "        (class_.start_frame, class_.num_frames, class_.frame_ratio, class_.frame_counter) = (\n",
    "            start_frame, num_frames, frame_ratio, num_frames + 1\n",
    "        )\n",
    "        return class_\n",
    "    \n",
    "    @classmethod\n",
    "    def read(class_) -> Iterator[ numpy.ndarray ]:\n",
    "        ratio = class_.frame_ratio\n",
    "        while True:\n",
    "            if (class_.num_frames < class_.frame_counter):\n",
    "                class_.frame_counter = 0\n",
    "                class_.video_stream.set(cv2.CAP_PROP_POS_FRAMES, class_.start_frame)\n",
    "            latest_frame = class_.video_stream.read()[1]\n",
    "            yield ((latest_frame) if (ratio == 1) else (cv2.resize(latest_frame, None, None, fx = ratio, fy = ratio)))\n",
    "            class_.frame_counter += 1\n",
    "            \n",
    "    @classmethod\n",
    "    def set_frame(class_, frame_number : int) -> \"VideoReader\":\n",
    "        \n",
    "        class_.video_stream.set(cv2.CAP_PROP_POS_FRAMES, frame_number)\n",
    "        \n",
    "        return class_\n",
    "    \n",
    "    @classmethod\n",
    "    def reset_counter(class_):\n",
    "        class_.frame_counter = class_.num_frames + 1\n",
    "            \n",
    "    @classmethod\n",
    "    def release(class_):\n",
    "        \n",
    "        class_.video_stream.release()\n",
    "        class_.video_stream  = None\n",
    "        class_.start_frame   = None\n",
    "        class_.num_frames    = None\n",
    "        class_.frame_ratio   = None\n",
    "        class_.frame_counter = None\n",
    "\n",
    "class FaceDetector:\n",
    "    \n",
    "    face_detector = None\n",
    "    \n",
    "    @classmethod\n",
    "    def initialize(class_, confidence_threshold : float, model_selection : int) -> \"FaceDetector\":\n",
    "        class_.face_detector = mediapipe.solutions.face_detection.FaceDetection(\n",
    "            model_selection = model_selection, min_detection_confidence = confidence_threshold\n",
    "        )\n",
    "        return class_\n",
    "    \n",
    "    @classmethod\n",
    "    def detect(class_, source_image : numpy.ndarray) -> List[ tuple ]:\n",
    "        def get_ratio_box(detection : mediapipe.framework.formats.detection_pb2.Detection) -> Tuple[ float ]:\n",
    "            relative_boundary_box = detection.location_data.relative_bounding_box\n",
    "            return (\n",
    "                relative_boundary_box.xmin,\n",
    "                relative_boundary_box.ymin,\n",
    "                relative_boundary_box.width,\n",
    "                relative_boundary_box.height\n",
    "            )\n",
    "        def rescale_boundary_box(boundary_box : Tuple[ float ], width : int, height : int) -> Tuple[ int ]:\n",
    "            return (\n",
    "                int(boundary_box[0] * width),\n",
    "                int(boundary_box[1] * height),\n",
    "                int(boundary_box[2] * width),\n",
    "                int(boundary_box[3] * height)\n",
    "            )\n",
    "        (height, width) = source_image.shape[:2]\n",
    "        detection_result = class_.face_detector.process(cv2.cvtColor(source_image, cv2.COLOR_BGR2RGB))\n",
    "        if (detection_result.detections is None):\n",
    "            return None\n",
    "        boundary_boxes = [\n",
    "            rescale_boundary_box(get_ratio_box(detection), width, height)\n",
    "                for detection in detection_result.detections\n",
    "        ]\n",
    "        return boundary_boxes\n",
    "    \n",
    "    @staticmethod\n",
    "    def convert_to_endpoint_boxes(boundary_boxes : List[ tuple ]) -> List[ tuple ]:\n",
    "        return [\n",
    "            (x, y, x + w, y + h) for (x, y, w, h) in boundary_boxes\n",
    "        ]\n",
    "    \n",
    "    @staticmethod\n",
    "    def convert_to_distance_boxes(boundary_boxes : List[ tuple ]) -> List[ tuple ]:\n",
    "        return [\n",
    "            (x, y, X - x, Y - y) for (x, y, X, Y) in boundary_boxes\n",
    "        ]\n",
    "    \n",
    "    @classmethod\n",
    "    def suppress_boundary_boxes(class_, boundary_boxes : List[ tuple ], threshold : float) -> List[ tuple ]:\n",
    "        boundary_boxes = numpy.array(class_.convert_to_endpoint_boxes(boundary_boxes))\n",
    "        return class_.convert_to_distance_boxes(\n",
    "            non_max_suppression(boundary_boxes, probs = None, overlapThresh = threshold))\n",
    "\n",
    "class MovementDetector:\n",
    "    \n",
    "    class List(list):\n",
    "        \n",
    "        def __init__(self):\n",
    "            super(MovementDetector.List, self).__init__(self)\n",
    "            \n",
    "        def pop_first(self):\n",
    "            if (self.__len__()):\n",
    "                return self.pop(0)\n",
    "            \n",
    "    def fetch_background(self, num_background_frames : int) -> numpy.ndarray:\n",
    "        \n",
    "        def fetch_frame(frame_number : int) -> numpy.ndarray:\n",
    "            return cv2.cvtColor(\n",
    "                next(VideoReader.set_frame(frame_number).read()), cv2.COLOR_BGR2GRAY)\n",
    "        \n",
    "        # fetch frames and resetting the frame counter when finished\n",
    "        def fetch_frames(frame_numbers : List[ int ]) -> numpy.ndarray:\n",
    "            initial_frame = VideoReader.video_stream.get(cv2.CAP_PROP_POS_FRAMES)\n",
    "            frames = numpy.stack([  fetch_frame(index) for index in frame_numbers  ])\n",
    "            VideoReader.set_frame(initial_frame)\n",
    "            VideoReader.reset_counter()\n",
    "            return frames\n",
    "        \n",
    "        # randomly select a certain number of frames within the loop\n",
    "        indices = numpy.random.choice(\n",
    "            range(VideoReader.start_frame, VideoReader.start_frame + VideoReader.num_frames), num_background_frames)\n",
    "        \n",
    "        # return the median of those randomly-fetched images\n",
    "        # median is used because it is more robust against outliers and eliminates a significantly amount of noises\n",
    "        return numpy.median(fetch_frames(indices), axis = 0)\n",
    "    \n",
    "    def __init__(self, num_background_frames : int) -> None:\n",
    "        \n",
    "        self.background_frame = numpy.uint8(self.fetch_background(num_background_frames))\n",
    "        self.frame_history = self.List()\n",
    "        \n",
    "        # the number of previous frames used to subtract with the background\n",
    "        self.trace_limit = 8\n",
    "        \n",
    "    def configure(self, trace_limit : int) -> \"MovementDetector\":\n",
    "        self.trace_limit = trace_limit\n",
    "        return self\n",
    "        \n",
    "    def append(self, latest_frame : numpy.ndarray) -> \"MovementDetector\":\n",
    "        \n",
    "        # convert the lastest frame to grayscale and add it to the frame history\n",
    "        self.frame_history.append(cv2.cvtColor(numpy.uint8(latest_frame), cv2.COLOR_BGR2GRAY))\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def pop_front(self) -> Union[ type(None), numpy.ndarray ]:\n",
    "        return self.frame_history.pop_first()\n",
    "    \n",
    "    def detect(self) -> numpy.ndarray:\n",
    "        \n",
    "        # detect movements obtained from background subtraction with the previous 8 frames in the queue\n",
    "        return numpy.sum(numpy.stack([\n",
    "            cv2.absdiff(self.background_frame, frame) \n",
    "                for frame in self.frame_history[-self.trace_limit:]\n",
    "        ]), axis = 0)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.frame_history.__len__()\n",
    "    \n",
    "    \n",
    "# because template matching performed poorly, the \"face recognition\" library is used\n",
    "# must install the library via \"pip install face_recognition\"\n",
    "# if there is an error concerning \"dlib\", download the wheel from PyPi / GitHub and install locally\n",
    "# after installing \"dlib\", retry \"pip install face_recognition\"\n",
    "class FaceRecognizer:\n",
    "    \n",
    "    @staticmethod\n",
    "    def find_identical_pair(faces : Union[ numpy.ndarray, List[numpy.ndarray], Tuple[numpy.ndarray] ]) -> Tuple[ tuple, float ]:\n",
    "        \n",
    "        encodings = [\n",
    "            face_recognition.face_encodings(faces[index]) for index in range(faces.__len__())\n",
    "        ]\n",
    "        \n",
    "        for pair in itertools.combinations(list(range(len(faces))), 2):\n",
    "            \n",
    "            first_image_encoding = encodings[pair[0]]\n",
    "            second_image_encoding = encodings[pair[1]]\n",
    "            if ((first_image_encoding.__len__() == 0) or (second_image_encoding.__len__() == 0)):\n",
    "                return\n",
    "            first_image_encoding = first_image_encoding[0]\n",
    "            second_image_encoding = second_image_encoding[0]\n",
    "            result = (face_recognition.compare_faces([ first_image_encoding ], second_image_encoding))\n",
    "            if (result):\n",
    "                return pair\n",
    "            \n",
    "    @classmethod\n",
    "    def find_identicals(class_, original_image : numpy.ndarray, boundary_boxes : Tuple[ tuple ]) -> Tuple[ tuple ]:\n",
    "        faces = [  original_image[ y : y + h, x : x + w ] for (x, y, w, h) in boundary_boxes  ]\n",
    "        if (faces.__len__() < 2):\n",
    "            return None\n",
    "        return class_.find_identical_pair(faces)\n",
    "    \n",
    "class ImageUtils:\n",
    "    \n",
    "    @staticmethod\n",
    "    def draw_boundary_boxes(display_frame : numpy.ndarray, \n",
    "            boundary_boxes : Union[ type(None), List[ tuple ]], color = (0, 0, 255), width = 5) -> numpy.ndarray:\n",
    "        \n",
    "        if (boundary_boxes is None):\n",
    "            return display_frame\n",
    "        \n",
    "        new_frame = numpy.copy(display_frame)\n",
    "        \n",
    "        for (x, y, w, h) in boundary_boxes:\n",
    "            new_frame = cv2.rectangle(new_frame, (x, y), (x + w, y + h), color, width)\n",
    "            \n",
    "        return new_frame\n",
    "    \n",
    "    @staticmethod\n",
    "    def enlarge_boundary_boxes(\n",
    "            boundary_boxes : List[ tuple ], ratio : float, max_height : int, max_width : int) -> List[ tuple ]:\n",
    "        \n",
    "        def enlarge(boundary_box : tuple) -> tuple:\n",
    "            (x, y, w, h) = boundary_box\n",
    "            (new_w, new_h) = (int(w * ratio), int(h * ratio))\n",
    "            (dif_w, dif_h) = (abs(w - new_w) // 2, abs(h - new_h) // 2)\n",
    "            (end_x, end_y) = (x + new_w, y + new_h)\n",
    "            return (\n",
    "                max(0, x - dif_w),\n",
    "                max(0, y - dif_h),\n",
    "                min(max_width, (end_x - x)),\n",
    "                min(max_height, (end_y - y))\n",
    "            )\n",
    "        return list(map(enlarge, boundary_boxes))\n",
    "\n",
    "# using thresholding and morphology, find the binary image for the cursor\n",
    "def compute_cursor_mask(current_colored_frame : numpy.ndarray) -> numpy.ndarray:\n",
    "    hsv_colored_frame = cv2.cvtColor(current_colored_frame, cv2.COLOR_BGR2HSV)\n",
    "    thresholded = cv2.inRange(hsv_colored_frame, (79, 69, 133), (128, 222, 286))\n",
    "    morph_list = [\n",
    "        lambda img : cv2.dilate(img, cv2.getStructuringElement(cv2.MORPH_ELLIPSE, ( 5,  5))                ),   \n",
    "        lambda img : cv2.erode(img,  cv2.getStructuringElement(cv2.MORPH_ELLIPSE, ( 7,  7))                ),   \n",
    "        lambda img : cv2.dilate(img, cv2.getStructuringElement(cv2.MORPH_ELLIPSE, ( 9,  9))                ),   \n",
    "        lambda img : cv2.erode(img,  cv2.getStructuringElement(cv2.MORPH_ELLIPSE, ( 6,  6)), iterations = 2),   \n",
    "        lambda img : cv2.dilate(img, cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (11, 11)), iterations = 2),    \n",
    "    ]\n",
    "    for morph in morph_list:\n",
    "        thresholded = morph(thresholded)\n",
    "    return numpy.uint8(thresholded)\n",
    "\n",
    "# using thresholding and morphology, find the binary image for the counter (timer UI)\n",
    "def compute_counter_mask(current_colored_frame : numpy.ndarray) -> numpy.ndarray:\n",
    "    hsv_colored_frame = cv2.cvtColor(current_colored_frame, cv2.COLOR_BGR2HSV)\n",
    "    thresholded = cv2.inRange(hsv_colored_frame, (0, 0, 44), (0, 4, 63))\n",
    "    morph_list = [\n",
    "        lambda img : cv2.dilate(img, cv2.getStructuringElement(cv2.MORPH_ELLIPSE, ( 5,  5))                ),   \n",
    "        lambda img : cv2.erode(img,  cv2.getStructuringElement(cv2.MORPH_ELLIPSE, ( 6,  6))                ),   \n",
    "        lambda img : cv2.dilate(img, cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (10, 10))                ),   \n",
    "        lambda img : cv2.erode(img,  cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (10, 10))                ),   \n",
    "        lambda img : cv2.dilate(img, cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (10, 10)), iterations = 2),\n",
    "        lambda img : cv2.erode(img,  cv2.getStructuringElement(cv2.MORPH_ELLIPSE, ( 7,  7)), iterations = 2),      \n",
    "        lambda img : cv2.dilate(img, cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (15, 15)), iterations = 2)\n",
    "    ]\n",
    "    for morph in morph_list:\n",
    "        thresholded = morph(thresholded)\n",
    "    return numpy.uint8(thresholded)\n",
    "\n",
    "# ignore movements from both the cursor and the counter (timer UI)\n",
    "def filter_difference(difference_mask : numpy.ndarray, \n",
    "        cursor_mask : numpy.ndarray, counter_mask : numpy.ndarray) -> numpy.ndarray:\n",
    "\n",
    "    # merge the two masks via \"bitwise_or\" to obtain a mask of regions in which to ignore movements\n",
    "    ignorable_mask = numpy.uint8(cv2.bitwise_or(cursor_mask, counter_mask))\n",
    "    \n",
    "    \"\"\"\n",
    "        Let A <-> all movements\n",
    "        Let B <-> all movements from cursor and timer\n",
    "        ----------------------------------------------\n",
    "        Truth Table:\n",
    "            A B C\n",
    "            0 0 0\n",
    "            0 1 0\n",
    "            1 0 1\n",
    "            1 1 0\n",
    "    \"\"\"\n",
    "    \n",
    "    # obtain a more accurate mask by following the aforementioned logic\n",
    "    return numpy.uint8(cv2.bitwise_and(\n",
    "        numpy.uint8(cv2.bitwise_not(ignorable_mask)),\n",
    "        numpy.uint8(cv2.bitwise_xor(ignorable_mask, difference_mask))\n",
    "    ))\n",
    "\n",
    "# obtain the bounding boxes of masked regions\n",
    "def mask_to_boundary_boxes(difference_mask : numpy.ndarray, thresh_1 : int, thresh_2 : int) -> numpy.ndarray:\n",
    "    \n",
    "    # find the contours of connected regions within the binary mask\n",
    "    contours = cv2.findContours(difference_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)[0]\n",
    "    \n",
    "    # convert contours to bounding boxes if the bounded area is greater than \"thresh_1\"\n",
    "    boxes = [  (x, y, w, h) for (x, y, w, h) in (cv2.boundingRect(cont) for cont in contours) if ((w * h) >= thresh_1) ]\n",
    "    \n",
    "    # return \"None\" if there are no bounding boxes which satisfy the above criteria\n",
    "    if (boxes.__len__() == 0):\n",
    "        return None\n",
    "    \n",
    "    # filter out bounding boxes whose bounded area is less than \"thresh_2\"\n",
    "    boxes = list(filter(lambda x : ((x[2] * x[3]) >= thresh_2), boxes))\n",
    "    \n",
    "    # the bounding boxes if there are some, and \"None\" otherwise\n",
    "    return ((boxes) if (boxes.__len__()) else (None))\n",
    "\n",
    "if (__name__ == \"__main__\"):\n",
    "    \n",
    "    first_frame          = 2180\n",
    "    \n",
    "    num_frames           = 200\n",
    "    \n",
    "    frame_ratio          = 0.75\n",
    "    \n",
    "    num_backgrounds      = 30\n",
    "    \n",
    "    trace_history        = 8\n",
    "    \n",
    "    confidence_threshold = 0.26\n",
    "    \n",
    "    model_selection      = 1\n",
    "    \n",
    "    difference_thresh    = 127\n",
    "    \n",
    "    comp_threshold_0     = 400\n",
    "    \n",
    "    comp_threshold_1     = 4600\n",
    "    \n",
    "    comp_threshold_2     = 6900\n",
    "    \n",
    "    overlap_thresh       = 0.01\n",
    "    \n",
    "    window_name          = \"find_two_look_alike\"\n",
    "    \n",
    "    video_file_name      = \"./wiiplay.mp4\"\n",
    "    \n",
    "    # load the video and initialize the frame range to loop\n",
    "    VideoReader.initialize(video_file_name).configure(first_frame, num_frames, frame_ratio)\n",
    "    \n",
    "    # initialize the mediapipe face detector\n",
    "    FaceDetector.initialize(confidence_threshold, model_selection)\n",
    "    \n",
    "    # initialize the movement detector based on background subtraction\n",
    "    movement_detector = MovementDetector(num_backgrounds).configure(trace_history)\n",
    "    \n",
    "    # create a named window\n",
    "    cv2.namedWindow(window_name)\n",
    "    \n",
    "    # endlessly read frames from video loop\n",
    "    for frame in VideoReader.read():\n",
    "        \n",
    "        # copy the current frame so as not to overwrite the original frame\n",
    "        display_frame = numpy.copy(frame)        \n",
    "        \n",
    "        # add the current frame to the movement detector\n",
    "        movement_detector.append(frame)\n",
    "        \n",
    "        # only start running the following after obtaining more than a certain number of frames\n",
    "        if (movement_detector.__len__() <= trace_history):\n",
    "            continue\n",
    "            \n",
    "        # through background subtraction, obtain the binary mask of regions with movements\n",
    "        difference_mask  = numpy.uint8(255 * (movement_detector.detect() >= difference_thresh))\n",
    "        \n",
    "        # obtain the counter (timer UI) mask via thresholding and morphology\n",
    "        counter_mask     = compute_counter_mask(frame)\n",
    "        \n",
    "        # obtain the cursor mask via thresholding and morphology\n",
    "        cursor_mask      = compute_cursor_mask(frame)\n",
    "        \n",
    "        # remove the oldest frame from the queue \n",
    "        movement_detector.pop_front()\n",
    "        \n",
    "        # obtain the final difference mask by ignoring movements of the cursor and the timer\n",
    "        difference_mask  = filter_difference(difference_mask, cursor_mask, counter_mask)\n",
    "        \n",
    "        # detect moving pedestrians according to the final mask and ignore movements less than thresholds\n",
    "        pedestrian_boxes = mask_to_boundary_boxes(difference_mask, comp_threshold_1, comp_threshold_2)\n",
    "        \n",
    "        # detect pedestrian faces using mediapipe's face detector and obtain their bounding boxes\n",
    "        face_boxes       = FaceDetector.detect(frame)\n",
    "        \n",
    "        # perform non maximum suppression on the bounding boxes if obtained\n",
    "        face_boxes       = ((face_boxes) if (face_boxes is None) \n",
    "                                else (FaceDetector.suppress_boundary_boxes(face_boxes, overlap_thresh)))\n",
    "\n",
    "        # use \"face_recongition\" to determine if any pair of faces are identical (their indices will be returned)\n",
    "        identical_boxes  = ((None) if (face_boxes is None) else (FaceRecognizer.find_identicals(frame, face_boxes)))\n",
    "        \n",
    "        # using indices, construct a new list of bounding boxes for the pair of identical faces\n",
    "        if (identical_boxes is not None):\n",
    "            \n",
    "            identical_boxes = [  face_boxes[i] for i in identical_boxes  ]\n",
    "        \n",
    "        # draw bounding boxes of pedestrians\n",
    "        display_frame    = ImageUtils.draw_boundary_boxes(display_frame, pedestrian_boxes, (0, 255, 0), 5)\n",
    "        \n",
    "        # draw bounding boxes of detected faces\n",
    "        display_frame    = ImageUtils.draw_boundary_boxes(display_frame, face_boxes,       (255, 0, 0), 4)\n",
    "        \n",
    "        # draw bounding boxes of identical faces on top of detected faces (width 5 > width 4)\n",
    "        display_frame    = ImageUtils.draw_boundary_boxes(display_frame, identical_boxes,  (0, 0, 255), 5)\n",
    "        \n",
    "        # display the resulting image\n",
    "        cv2.imshow(window_name, display_frame)\n",
    "        \n",
    "        # terminate the program if Esc is pressed\n",
    "        if (cv2.waitKey(1) == 27):\n",
    "            break\n",
    "            \n",
    "    cv2.destroyWindow(window_name)\n",
    "    \n",
    "    VideoReader.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\VIN\\AppData\\Roaming\\Python\\Python37\\site-packages\\numpy\\core\\fromnumeric.py:3441: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "C:\\Users\\VIN\\AppData\\Roaming\\Python\\Python37\\site-packages\\numpy\\core\\_methods.py:189: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    }
   ],
   "source": [
    "# problem 3\n",
    "# frames 2481 - 2601 ( lvl9 )\n",
    "# detect pedestrians and draw blue boxes on first frame, track pedestrians and draw green boxes, draw red box on fastest mii\n",
    "\n",
    "from typing import Iterator, Union, Tuple, List\n",
    "from imutils.object_detection import non_max_suppression\n",
    "import mediapipe, numpy, cv2\n",
    "\n",
    "class VideoReader:\n",
    "    \n",
    "    video_stream  = None\n",
    "    start_frame   = None\n",
    "    num_frames    = None\n",
    "    frame_ratio   = None\n",
    "    frame_counter = None\n",
    "    \n",
    "    @classmethod\n",
    "    def initialize(class_, video_file_name : str) -> \"VideoReader\":\n",
    "        class_.video_stream = cv2.VideoCapture(video_file_name)\n",
    "        if not (class_.video_stream.isOpened()):\n",
    "            raise IOError(\"Cannot open video file \\\"{}\\\"\\n\".format(video_file_name))\n",
    "        return class_\n",
    "    \n",
    "    @classmethod\n",
    "    def configure(class_, start_frame : int, num_frames : int, frame_ratio : float = 1) -> \"VideoReader\":\n",
    "        (class_.start_frame, class_.num_frames, class_.frame_ratio, class_.frame_counter) = (\n",
    "            start_frame, num_frames, frame_ratio, num_frames + 1\n",
    "        )\n",
    "        return class_\n",
    "    \n",
    "    @classmethod\n",
    "    def read(class_) -> Iterator[ numpy.ndarray ]:\n",
    "        ratio = class_.frame_ratio\n",
    "        while True:\n",
    "            if (class_.num_frames < class_.frame_counter):\n",
    "                class_.frame_counter = 0\n",
    "                class_.video_stream.set(cv2.CAP_PROP_POS_FRAMES, class_.start_frame)\n",
    "            latest_frame = class_.video_stream.read()[1]\n",
    "            yield ((latest_frame) if (ratio == 1) else (cv2.resize(latest_frame, None, None, fx = ratio, fy = ratio)))\n",
    "            class_.frame_counter += 1\n",
    "            \n",
    "    @classmethod\n",
    "    def set_frame(class_, frame_number : int) -> \"VideoReader\":\n",
    "        \n",
    "        class_.video_stream.set(cv2.CAP_PROP_POS_FRAMES, frame_number)\n",
    "        \n",
    "        return class_\n",
    "    \n",
    "    @classmethod\n",
    "    def reset_counter(class_):\n",
    "        class_.frame_counter = class_.num_frames + 1\n",
    "            \n",
    "    @classmethod\n",
    "    def release(class_):\n",
    "        \n",
    "        class_.video_stream.release()\n",
    "        class_.video_stream  = None\n",
    "        class_.start_frame   = None\n",
    "        class_.num_frames    = None\n",
    "        class_.frame_ratio   = None\n",
    "        class_.frame_counter = None\n",
    "\n",
    "class FaceDetector:\n",
    "    \n",
    "    face_detector = None\n",
    "    \n",
    "    @classmethod\n",
    "    def initialize(class_, confidence_threshold : float, model_selection : int) -> \"FaceDetector\":\n",
    "        class_.face_detector = mediapipe.solutions.face_detection.FaceDetection(\n",
    "            model_selection = model_selection, min_detection_confidence = confidence_threshold\n",
    "        )\n",
    "        return class_\n",
    "    \n",
    "    @classmethod\n",
    "    def detect(class_, source_image : numpy.ndarray) -> List[ tuple ]:\n",
    "        def get_ratio_box(detection : mediapipe.framework.formats.detection_pb2.Detection) -> Tuple[ float ]:\n",
    "            relative_boundary_box = detection.location_data.relative_bounding_box\n",
    "            return (\n",
    "                relative_boundary_box.xmin,\n",
    "                relative_boundary_box.ymin,\n",
    "                relative_boundary_box.width,\n",
    "                relative_boundary_box.height\n",
    "            )\n",
    "        def rescale_boundary_box(boundary_box : Tuple[ float ], width : int, height : int) -> Tuple[ int ]:\n",
    "            return (\n",
    "                int(boundary_box[0] * width),\n",
    "                int(boundary_box[1] * height),\n",
    "                int(boundary_box[2] * width),\n",
    "                int(boundary_box[3] * height)\n",
    "            )\n",
    "        (height, width) = source_image.shape[:2]\n",
    "        detection_result = class_.face_detector.process(cv2.cvtColor(source_image, cv2.COLOR_BGR2RGB))\n",
    "        if (detection_result.detections is None):\n",
    "            return None\n",
    "        boundary_boxes = [\n",
    "            rescale_boundary_box(get_ratio_box(detection), width, height)\n",
    "                for detection in detection_result.detections\n",
    "        ]\n",
    "        return boundary_boxes\n",
    "    \n",
    "    @staticmethod\n",
    "    def convert_to_endpoint_boxes(boundary_boxes : List[ tuple ]) -> List[ tuple ]:\n",
    "        return [\n",
    "            (x, y, x + w, y + h) for (x, y, w, h) in boundary_boxes\n",
    "        ]\n",
    "    \n",
    "    @staticmethod\n",
    "    def convert_to_distance_boxes(boundary_boxes : List[ tuple ]) -> List[ tuple ]:\n",
    "        return [\n",
    "            (x, y, X - x, Y - y) for (x, y, X, Y) in boundary_boxes\n",
    "        ]\n",
    "    \n",
    "    @classmethod\n",
    "    def suppress_boundary_boxes(class_, boundary_boxes : List[ tuple ], threshold : float) -> List[ tuple ]:\n",
    "        boundary_boxes = numpy.array(class_.convert_to_endpoint_boxes(boundary_boxes))\n",
    "        return class_.convert_to_distance_boxes(\n",
    "            non_max_suppression(boundary_boxes, probs = None, overlapThresh = threshold))\n",
    "\n",
    "class ImageUtils:\n",
    "    \n",
    "    @staticmethod\n",
    "    def draw_boundary_boxes(display_frame : numpy.ndarray, \n",
    "            boundary_boxes : Union[ type(None), List[ tuple ]], color = (0, 0, 255), width = 5) -> numpy.ndarray:\n",
    "        \n",
    "        if (boundary_boxes is None):\n",
    "            return display_frame\n",
    "        \n",
    "        new_frame = numpy.copy(display_frame)\n",
    "        \n",
    "        for (x, y, w, h) in boundary_boxes:\n",
    "            new_frame = cv2.rectangle(new_frame, (x, y), (x + w, y + h), color, width)\n",
    "            \n",
    "        return new_frame\n",
    "    \n",
    "    @staticmethod\n",
    "    def enlarge_boundary_boxes(\n",
    "            boundary_boxes : List[ tuple ], ratio : float, max_height : int, max_width : int) -> List[ tuple ]:\n",
    "        \n",
    "        def enlarge(boundary_box : tuple) -> tuple:\n",
    "            (x, y, w, h) = boundary_box\n",
    "            (new_w, new_h) = (int(w * ratio), int(h * ratio))\n",
    "            (dif_w, dif_h) = (abs(w - new_w) // 2, abs(h - new_h) // 2)\n",
    "            (end_x, end_y) = (x + new_w, y + new_h)\n",
    "            return (\n",
    "                max(0, x - dif_w),\n",
    "                max(0, y - dif_h),\n",
    "                min(max_width, (end_x - x)),\n",
    "                min(max_height, (end_y - y))\n",
    "            )\n",
    "        return list(map(enlarge, boundary_boxes))\n",
    "    \n",
    "class TrackerList:\n",
    "    \n",
    "    def __init__(self, reference_frame : numpy.ndarray, boundary_boxes : List[ tuple ]) -> \"TrackerList\":\n",
    "        \n",
    "        # [ [ \"index\", \"tracker object\", \"previous centroid\", \"distance history\", \"flag\" ] ]\n",
    "        self.trackerList = [  [index, cv2.TrackerKCF_create(), (None, None), [], 0] for index in range(len(boundary_boxes))  ]\n",
    "        \n",
    "        # initialize the KCF tracker for each pedestrian\n",
    "        # KCF is used because it is approximately ten times faster than MIL\n",
    "        for index, tracker, *_ in self.trackerList:\n",
    "            tracker.init(reference_frame, boundary_boxes[index])\n",
    "            \n",
    "    def update(self, latest_frame : numpy.ndarray, filter_lost : bool = False) -> List[ tuple ]:\n",
    "               \n",
    "        # update the trackers for each pedestrian by passing them the latest frame\n",
    "        updated_boxes = [  (index, tracker.update(latest_frame)) for index, tracker, *_ in self.trackerList  ]\n",
    "        \n",
    "        # initialize an empty list used to store the median of each pedestrian's speed history\n",
    "        # \"median\" is considered more robust than \"mean\" (average) when there are outliers\n",
    "        medians = []\n",
    "        \n",
    "        # calculate the median of each pedestrian's speed history after update\n",
    "        for index, updated_box in updated_boxes:\n",
    "            \n",
    "            # obtain the center of the bounding box\n",
    "            latest_centroid = self.compute_centroids(*updated_box[1])\n",
    "            \n",
    "            # fetch the previous center of the bounding box\n",
    "            previous_centroid = self.trackerList[index][2]\n",
    "            \n",
    "            # only compute the displacement if previous centroids exist\n",
    "            if (previous_centroid != (None, None)):\n",
    "                latest_distance = self.compute_distance_ss(latest_centroid, previous_centroid)\n",
    "                self.trackerList[index][3].append(latest_distance)\n",
    "                \n",
    "            # calculate the median of each pedestrian's speed history\n",
    "            median = numpy.median(self.trackerList[index][3])\n",
    "            \n",
    "            # add the new median to list\n",
    "            medians.append(median)\n",
    "            \n",
    "            # update the latest center point\n",
    "            self.trackerList[index][2] = latest_centroid\n",
    "            \n",
    "        # find the fastest pedestrian of the bunch\n",
    "        fastest = numpy.argmax(medians)\n",
    "                \n",
    "        # whether or not to consider trackers which lost track\n",
    "        if (filter_lost):\n",
    "            updated_boxes = list(filter(lambda x : x[1][0], updated_boxes))\n",
    "            \n",
    "        # format the data structure and return the result\n",
    "        return list(map(lambda x : (x[0], x[1][1], (x[0] == fastest)), updated_boxes))\n",
    "    \n",
    "    @staticmethod\n",
    "    def compute_centroids(x : int, y : int, w : int, h : int) -> Tuple[ int ]:\n",
    "        \n",
    "        # calculate the center point of this particular bounding box\n",
    "        return (x + w // 2, y + h // 2)\n",
    "    \n",
    "    @staticmethod\n",
    "    def compute_distance_ss(coordinates_1 : Tuple[ int ], coordinates_2 : Tuple[ int ]) -> float:\n",
    "        \n",
    "        # calculate the displacement using Manhattan distance\n",
    "        # Manhattan distance is used instead of Euclidean distance because it saves time\n",
    "        return numpy.sum(numpy.abs(numpy.array(coordinates_1) - numpy.array(coordinates_2)))\n",
    "        \n",
    "class PeopleDetector:\n",
    "    \n",
    "    people_detector = None\n",
    "    \n",
    "    @classmethod\n",
    "    def initialize(class_, frame : numpy.ndarray) -> \"PeopleDetector\":\n",
    "        class_.frame = frame\n",
    "        class_.people_detector = cv2.HOGDescriptor()\n",
    "        class_.people_detector.setSVMDetector(cv2.HOGDescriptor_getDefaultPeopleDetector())\n",
    "        return class_\n",
    "    \n",
    "    @classmethod\n",
    "    def detect(class_, threshold : float = 0.01, win_stride : tuple = (5, 5), ** kwargs) -> Union[ tuple, numpy.ndarray ]:\n",
    "        return class_.people_detector.detectMultiScale(\n",
    "            class_.frame, hitThreshold = threshold, winStride = win_stride, ** kwargs)[0]\n",
    "\n",
    "if (__name__ == \"__main__\"):\n",
    "    \n",
    "    video_file_name = \"./wiiplay.mp4\"\n",
    "    \n",
    "    start_frame     = 2481\n",
    "    \n",
    "    num_frames      = 120\n",
    "    \n",
    "    frame_ratio     = 1\n",
    "    \n",
    "    threshold       = 0.5\n",
    "    \n",
    "    window_name     = \"find_the_fastest_character\"\n",
    "    \n",
    "    # load the video and initialize the frame range to loop\n",
    "    VideoReader.initialize(video_file_name).configure(start_frame, num_frames, frame_ratio)\n",
    "    \n",
    "    # obtain the first frame of the loop\n",
    "    template_frame          = next(VideoReader.read())\n",
    "    \n",
    "    # detect pedestrians using the HOG detector and perform non maximum suppression\n",
    "    template_boundary_boxes = FaceDetector.suppress_boundary_boxes(\n",
    "        PeopleDetector.initialize(template_frame).detect(), threshold)\n",
    "    \n",
    "    # initialize a history list to track each bounding box as the video progresses\n",
    "    trackerList             = TrackerList(template_frame, template_boundary_boxes)\n",
    "    \n",
    "    # initialize an output window\n",
    "    cv2.namedWindow(window_name)\n",
    "    \n",
    "    # draw blue bounding boxes on the first frame\n",
    "    template_frame = ImageUtils.draw_boundary_boxes(template_frame, template_boundary_boxes, (255, 0, 0), 5)\n",
    "    \n",
    "    # display the first frame containing detected pedestrians and await keyboard movements to continue\n",
    "    cv2.imshow(window_name, template_frame);  cv2.waitKey(0)\n",
    "    \n",
    "    # endlessly read frames from video loop until Esc is pressed or we wish to stop\n",
    "    for frame in VideoReader.read():\n",
    "        \n",
    "        # for each pedestrian, obtain the updated bounding box and a flag of whether it is the fastest\n",
    "        for index_label, bbox, fastest in trackerList.update(frame, True):\n",
    "\n",
    "            # draw red rectangles if it is the fastest; otherwise, draw green rectangles\n",
    "            color        = ((0, 0, 255) if (fastest) else (0, 255, 0))\n",
    "            \n",
    "            # unpack values from the bounding box\n",
    "            (x, y, w, h) = [  int(i) for i in bbox  ]\n",
    "            \n",
    "            # draw the bounding box\n",
    "            frame        = cv2.rectangle(frame, (x, y), (x + w, y + h), color, 5)\n",
    "        \n",
    "        # display the resulting image\n",
    "        cv2.imshow(window_name, frame)\n",
    "        \n",
    "        # terminate the program if Esc is pressed or the first loop has ended\n",
    "        if ((cv2.waitKey(1) == 27) or ((VideoReader.num_frames - 1) < VideoReader.frame_counter)):\n",
    "            break\n",
    "            \n",
    "    cv2.destroyWindow(window_name)\n",
    "            \n",
    "    VideoReader.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# problem 4 \n",
    "# frames 1650 - 1800 ( lvl6 )\n",
    "# with blue arrows draw optical flow, detect two people facing different direction and draw red rectangles\n",
    "\n",
    "from typing import Iterator, Union, Tuple, List\n",
    "from imutils.object_detection import non_max_suppression\n",
    "import mediapipe, numpy, cv2\n",
    "\n",
    "class VideoReader:\n",
    "    \n",
    "    video_stream  = None\n",
    "    start_frame   = None\n",
    "    num_frames    = None\n",
    "    frame_ratio   = None\n",
    "    frame_counter = None\n",
    "    \n",
    "    @classmethod\n",
    "    def initialize(class_, video_file_name : str) -> \"VideoReader\":\n",
    "        class_.video_stream = cv2.VideoCapture(video_file_name)\n",
    "        if not (class_.video_stream.isOpened()):\n",
    "            raise IOError(\"Cannot open video file \\\"{}\\\"\\n\".format(video_file_name))\n",
    "        return class_\n",
    "    \n",
    "    @classmethod\n",
    "    def configure(class_, start_frame : int, num_frames : int, frame_ratio : float = 1) -> \"VideoReader\":\n",
    "        (class_.start_frame, class_.num_frames, class_.frame_ratio, class_.frame_counter) = (\n",
    "            start_frame, num_frames, frame_ratio, num_frames + 1\n",
    "        )\n",
    "        return class_\n",
    "    \n",
    "    @classmethod\n",
    "    def read(class_) -> Iterator[ numpy.ndarray ]:\n",
    "        ratio = class_.frame_ratio\n",
    "        while True:\n",
    "            if (class_.num_frames < class_.frame_counter):\n",
    "                class_.frame_counter = 0\n",
    "                class_.video_stream.set(cv2.CAP_PROP_POS_FRAMES, class_.start_frame)\n",
    "            latest_frame = class_.video_stream.read()[1]\n",
    "            yield ((latest_frame) if (ratio == 1) else (cv2.resize(latest_frame, None, None, fx = ratio, fy = ratio)))\n",
    "            class_.frame_counter += 1\n",
    "            \n",
    "    @classmethod\n",
    "    def set_frame(class_, frame_number : int) -> \"VideoReader\":\n",
    "        \n",
    "        class_.video_stream.set(cv2.CAP_PROP_POS_FRAMES, frame_number)\n",
    "        \n",
    "        return class_\n",
    "    \n",
    "    @classmethod\n",
    "    def reset_counter(class_):\n",
    "        class_.frame_counter = class_.num_frames + 1\n",
    "            \n",
    "    @classmethod\n",
    "    def release(class_):\n",
    "        \n",
    "        class_.video_stream.release()\n",
    "        class_.video_stream  = None\n",
    "        class_.start_frame   = None\n",
    "        class_.num_frames    = None\n",
    "        class_.frame_ratio   = None\n",
    "        class_.frame_counter = None\n",
    "\n",
    "class BasicFaceDetector:\n",
    "    \n",
    "    face_detector = None\n",
    "    detector_file_name = \"./haarcascade_frontalface_alt2.xml\"\n",
    "    \n",
    "    @classmethod\n",
    "    def initialize(class_) -> \"BasicFaceDetector\":\n",
    "        class_.face_detector = cv2.CascadeClassifier(class_.detector_file_name)\n",
    "        if (class_.face_detector.empty()):\n",
    "            raise IOError(\"Cannot find XML file \\\"{}\\\"\\n\".format(class_.detector_file_name))\n",
    "        return class_\n",
    "    \n",
    "    @classmethod\n",
    "    def detect(class_, source_image : numpy.ndarray, ** kwargs) -> Union[ numpy.ndarray, tuple ]:\n",
    "        return class_.face_detector.detectMultiScale(source_image, ** kwargs)\n",
    "\n",
    "class FaceDetector:\n",
    "    \n",
    "    face_detector = None\n",
    "    \n",
    "    @classmethod\n",
    "    def initialize(class_, confidence_threshold : float, model_selection : int) -> \"FaceDetector\":\n",
    "        class_.face_detector = mediapipe.solutions.face_detection.FaceDetection(\n",
    "            model_selection = model_selection, min_detection_confidence = confidence_threshold\n",
    "        )\n",
    "        return class_\n",
    "    \n",
    "    @classmethod\n",
    "    def detect(class_, source_image : numpy.ndarray) -> List[ tuple ]:\n",
    "        def get_ratio_box(detection : mediapipe.framework.formats.detection_pb2.Detection) -> Tuple[ float ]:\n",
    "            relative_boundary_box = detection.location_data.relative_bounding_box\n",
    "            return (\n",
    "                relative_boundary_box.xmin,\n",
    "                relative_boundary_box.ymin,\n",
    "                relative_boundary_box.width,\n",
    "                relative_boundary_box.height\n",
    "            )\n",
    "        def rescale_boundary_box(boundary_box : Tuple[ float ], width : int, height : int) -> Tuple[ int ]:\n",
    "            return (\n",
    "                int(boundary_box[0] * width),\n",
    "                int(boundary_box[1] * height),\n",
    "                int(boundary_box[2] * width),\n",
    "                int(boundary_box[3] * height)\n",
    "            )\n",
    "        (height, width) = source_image.shape[:2]\n",
    "        detection_result = class_.face_detector.process(cv2.cvtColor(source_image, cv2.COLOR_BGR2RGB))\n",
    "        if (detection_result.detections is None):\n",
    "            return None\n",
    "        boundary_boxes = [\n",
    "            rescale_boundary_box(get_ratio_box(detection), width, height)\n",
    "                for detection in detection_result.detections\n",
    "        ]\n",
    "        return boundary_boxes\n",
    "    \n",
    "    @staticmethod\n",
    "    def convert_to_endpoint_boxes(boundary_boxes : List[ tuple ]) -> List[ tuple ]:\n",
    "        return [\n",
    "            (x, y, x + w, y + h) for (x, y, w, h) in boundary_boxes\n",
    "        ]\n",
    "    \n",
    "    @staticmethod\n",
    "    def convert_to_distance_boxes(boundary_boxes : List[ tuple ]) -> List[ tuple ]:\n",
    "        return [\n",
    "            (x, y, X - x, Y - y) for (x, y, X, Y) in boundary_boxes\n",
    "        ]\n",
    "    \n",
    "    @classmethod\n",
    "    def suppress_boundary_boxes(class_, boundary_boxes : List[ tuple ], threshold : float) -> List[ tuple ]:\n",
    "        boundary_boxes = numpy.array(class_.convert_to_endpoint_boxes(boundary_boxes))\n",
    "        return class_.convert_to_distance_boxes(\n",
    "            non_max_suppression(boundary_boxes, probs = None, overlapThresh = threshold))\n",
    "        \n",
    "class ImageUtils:\n",
    "    \n",
    "    @staticmethod\n",
    "    def draw_boundary_boxes(display_frame : numpy.ndarray, \n",
    "            boundary_boxes : Union[ type(None), List[ tuple ]], color = (0, 0, 255), width = 5) -> numpy.ndarray:\n",
    "        \n",
    "        if (boundary_boxes is None):\n",
    "            return display_frame\n",
    "        \n",
    "        new_frame = numpy.copy(display_frame)\n",
    "        \n",
    "        for (x, y, w, h) in boundary_boxes:\n",
    "            new_frame = cv2.rectangle(new_frame, (x, y), (x + w, y + h), color, width)\n",
    "            \n",
    "        return new_frame\n",
    "    \n",
    "    @staticmethod\n",
    "    def enlarge_boundary_boxes(\n",
    "            boundary_boxes : List[ tuple ], ratio : float, max_height : int, max_width : int) -> List[ tuple ]:\n",
    "        \n",
    "        def enlarge(boundary_box : tuple) -> tuple:\n",
    "            (x, y, w, h) = boundary_box\n",
    "            (new_w, new_h) = (int(w * ratio), int(h * ratio))\n",
    "            (dif_w, dif_h) = (abs(w - new_w) // 2, abs(h - new_h) // 2)\n",
    "            (end_x, end_y) = (x + new_w, y + new_h)\n",
    "            return (\n",
    "                max(0, x - dif_w),\n",
    "                max(0, y - dif_h),\n",
    "                min(max_width, (end_x - x)),\n",
    "                min(max_height, (end_y - y))\n",
    "            )\n",
    "        return list(map(enlarge, boundary_boxes))\n",
    "        \n",
    "# LEFT-labeled direction is colored \"blue\"\n",
    "LEFT  = (255, 0, 0)\n",
    "\n",
    "# RIGHT-labeled direction is colored \"red\"\n",
    "RIGHT = (0, 0, 255)\n",
    "\n",
    "class OpticalDetector:\n",
    "    \n",
    "    previous_frame = None\n",
    "    \n",
    "    optical_frame  = None\n",
    "    \n",
    "    @classmethod\n",
    "    def initialize(class_) -> \"OpticalDetector\":\n",
    "        \n",
    "        class_.previous_frame = None\n",
    "        class_.optical_frame  = None\n",
    "        \n",
    "        return class_\n",
    "    \n",
    "    @classmethod\n",
    "    def find_optical_frame(class_, latest_frame : numpy.ndarray) -> numpy.ndarray:\n",
    "        \n",
    "        # convert the latest frame to grayscale\n",
    "        latest_frame = cv2.cvtColor(latest_frame, cv2.COLOR_BGR2GRAY)\n",
    "        \n",
    "        # only try to compute the optical frame if previous frame exists\n",
    "        if (class_.previous_frame is not None):\n",
    "            \n",
    "            # obtain the optical frame \n",
    "            class_.optical_frame = cv2.calcOpticalFlowFarneback(\n",
    "                class_.previous_frame, latest_frame, \n",
    "                class_.optical_frame, 0.5, 5, 13, 10, 5, 1.1, \n",
    "                cv2.OPTFLOW_FARNEBACK_GAUSSIAN\n",
    "            )\n",
    "            \n",
    "        # update the previous frame\n",
    "        class_.previous_frame = numpy.copy(latest_frame)\n",
    "        \n",
    "        return class_.optical_frame\n",
    "    \n",
    "# obtain the direction in which the minority (odd pedestrians) are facing \n",
    "def get_min_direction(faces_mask : numpy.ndarray, boundary_boxes : List[ tuple ]) -> int:\n",
    "    \n",
    "    directions = []\n",
    "    \n",
    "    for (x, y, w, h) in boundary_boxes:\n",
    "        \n",
    "        # crop the image given a bounding box\n",
    "        image  = faces_mask[ y : y + h, x : x + w ]\n",
    "        \n",
    "        # calculate the number of arrow pixels of the first direction\n",
    "        first  = numpy.where((image ==  LEFT))[0].__len__()\n",
    "        \n",
    "        # calculate the number of arrow pixels of the second direction\n",
    "        second = numpy.where((image == RIGHT))[0].__len__()\n",
    "        \n",
    "        # save the greater of the two\n",
    "        directions.append(((LEFT) if (first > second) else (RIGHT)))\n",
    "        \n",
    "    # calculate the number of pedestrians facing the first direction\n",
    "    first  = sum(  (direction ==  LEFT) for direction in directions  )\n",
    "    \n",
    "    # calculate the number of pedestrians facing the second direction\n",
    "    second = sum(  (direction == RIGHT) for direction in directions  )\n",
    "    \n",
    "    # return the greater direction and the number of pedestrians\n",
    "    return ((LEFT, first) if (first < second) else (RIGHT, second))\n",
    "        \n",
    "if (__name__ == \"__main__\"):\n",
    "    \n",
    "    video_file_name = \"./wiiplay.mp4\"\n",
    "    \n",
    "    start_frame     = 1650\n",
    "    \n",
    "    num_frames      = 150\n",
    "    \n",
    "    model_selection = 1\n",
    "    \n",
    "    threshold       = 0.32\n",
    "    \n",
    "    frame_ratio     = 0.5\n",
    "    \n",
    "    enlarge_ratio   = 1.3\n",
    "    \n",
    "    pixel_thresh    = 3000\n",
    "    \n",
    "    strides         = 10\n",
    "    \n",
    "    window_name     = \"find_two_odds\"\n",
    "    \n",
    "    # load the video and initialize the frame range to loop\n",
    "    VideoReader.initialize(video_file_name).configure(start_frame, num_frames, frame_ratio)\n",
    "    \n",
    "    # initialize the haarcascade face detector (must use \"haarcascade_frontalface_alt2.xml\")\n",
    "    # should download the cascade classifier from the official GitHub repository\n",
    "    BasicFaceDetector.initialize()\n",
    "    \n",
    "    # initialize the optical flow detector \n",
    "    OpticalDetector.initialize()\n",
    "    \n",
    "    cv2.namedWindow(window_name)\n",
    "    \n",
    "    # endlessly read frames from the video until Esc is pressed\n",
    "    for frame in VideoReader.read():\n",
    "\n",
    "        display_frame   = numpy.copy(frame)\n",
    "        \n",
    "        # obtain the frame of optical flow\n",
    "        optical_frame   = OpticalDetector.find_optical_frame(frame)\n",
    "        \n",
    "        # initialize an empty mask\n",
    "        direction_frame = numpy.zeros(shape = display_frame.shape, dtype = numpy.uint8)\n",
    "        \n",
    "        # detect faces from the current frame using the haarcascade face detector\n",
    "        faces           = BasicFaceDetector.detect(frame)\n",
    "        \n",
    "        # perform non maximum suppression on the bounding boxes if there are detected faces\n",
    "        faces           = ((faces) if (faces is None) else (FaceDetector.suppress_boundary_boxes(faces, threshold)))\n",
    "        \n",
    "        # enlarge the bounding boxes by a certain ratio if there are bounding boxes of faces\n",
    "        if (faces is not None):\n",
    "            \n",
    "            faces = ImageUtils.enlarge_boundary_boxes(faces, enlarge_ratio, * display_frame.shape[ : 2])\n",
    "        \n",
    "        # obtain a binary mask of faces by drawing solid (white) rectangles on an empty (black) mask\n",
    "        faces_mask      = ImageUtils.draw_boundary_boxes(\n",
    "            numpy.zeros(shape = display_frame.shape, dtype = numpy.uint8), faces, color = (255, 255, 255), width = -1)\n",
    "\n",
    "        if (optical_frame is not None):\n",
    "            \n",
    "            for index in numpy.ndindex(optical_frame[::strides, ::strides].shape[:2]):\n",
    "                \n",
    "                # obtain the first point\n",
    "                pt1   = tuple(i * strides for i in index)\n",
    "                \n",
    "                # obtain the displacement\n",
    "                delta = numpy.int32(optical_frame[pt1])[::-1]\n",
    "                \n",
    "                # obtain the second point\n",
    "                pt2   = tuple(pt1 + 2 * delta)\n",
    "                \n",
    "                # only consider cases where \"delta\" is within a certain range\n",
    "                if (1 <= cv2.norm(delta) <= 10):\n",
    "\n",
    "                    # use different colors for arrows pointing in opposite directions\n",
    "                    direction = ((LEFT) if (pt1[::-1] > pt2[::-1]) else (RIGHT))\n",
    "                    \n",
    "                    # draw different colored arrows on the direction mask \n",
    "                    cv2.arrowedLine(direction_frame, pt1[::-1], pt2[::-1],   direction, 1, cv2.LINE_AA, 0, 0.01)\n",
    "                    \n",
    "                    # draw blue arrows on the output image to show the optical flow \n",
    "                    cv2.arrowedLine(display_frame,   pt1[::-1], pt2[::-1], (255, 0, 0), 1, cv2.LINE_AA, 0, 0.01)\n",
    "        \n",
    "            # only consider the optical flow of face regions\n",
    "            faces_mask      = numpy.where((faces_mask == (255, 255, 255)), direction_frame, faces_mask)\n",
    "            \n",
    "            # as there are only two odd people, finding the direction in which few people are facing (turning from/to)\n",
    "            direction, num  = get_min_direction(faces_mask, faces)\n",
    "            \n",
    "            # only consider the case if at least two people facing in odd directions are detected\n",
    "            if (num > 1):\n",
    "            \n",
    "                for (x, y, w, h) in faces:\n",
    "\n",
    "                    # calculate the number of arrows of the first color (first direction)\n",
    "                    first  = numpy.where((faces_mask[ y : y + h, x : x + w ] ==  LEFT))[0].__len__()\n",
    "\n",
    "                    # calculate the number of arrows of the second color (second direction)\n",
    "                    second = numpy.where((faces_mask[ y : y + h, x : x + w ] == RIGHT))[0].__len__()\n",
    "\n",
    "                    # determine the direction in which this pedestrian is facing \n",
    "                    direct = ((RIGHT) if ((first > pixel_thresh) and (second > pixel_thresh) and (second > first)) else (LEFT))\n",
    "\n",
    "                    # draw red rectangles on odd pedestrians if the number of arrow pixels surpass a certain threshold\n",
    "                    if ((first > pixel_thresh) and (second > pixel_thresh) and (direct == direction)):\n",
    "\n",
    "                        display_frame = cv2.rectangle(display_frame, (x, y), (x + w, y + h), (0, 0, 255), 5)\n",
    "        \n",
    "        # display the resulting image\n",
    "        cv2.imshow(window_name, display_frame)\n",
    "        \n",
    "        # terminate the program if Esc is pressed\n",
    "        if (cv2.waitKey(1) == 27):\n",
    "            break\n",
    "            \n",
    "    cv2.destroyWindow(window_name)\n",
    "            \n",
    "    VideoReader.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# problem 5\n",
    "# continuous frames from webcam\n",
    "# detect 21 finger joints, and determine whether posture is rock, paper or scissor\n",
    "\n",
    "from typing import Iterator, Union, Tuple, Dict, List\n",
    "import mediapipe, numpy, math, cv2\n",
    "\n",
    "class HandDetector:\n",
    "    \n",
    "    hand_drawer    = None\n",
    "    hand_detector  = None\n",
    "    hand_processor = None\n",
    "    \n",
    "    @classmethod\n",
    "    def initialize(class_) -> \"HandDetector\":\n",
    "        class_.hand_drawer   = mediapipe.solutions.drawing_utils\n",
    "        class_.hand_detector = mediapipe.solutions.hands\n",
    "        return class_\n",
    "    \n",
    "    @classmethod\n",
    "    def configure(class_, min_detection_confidence : float = 0.7, min_tracking_confidence : float = 0.7) -> \"HandDetector\":\n",
    "        class_.hand_processor = class_.hand_detector.Hands(\n",
    "            min_detection_confidence = min_detection_confidence, \n",
    "            min_tracking_confidence  = min_tracking_confidence\n",
    "        )\n",
    "        return class_\n",
    "    \n",
    "    @classmethod\n",
    "    def detect(class_, frame : numpy.ndarray) -> \"mediapipe.python.solution_base.SolutionOutputs\":\n",
    "        \n",
    "        # convert the image to grayscale and detect hands\n",
    "        return class_.hand_processor.process(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "    \n",
    "    @classmethod\n",
    "    def draw_landmarks(class_, source_image : numpy.ndarray, \n",
    "            hand_landmarks : \"mediapipe.python.solution_base.SolutionOutputs\") -> numpy.ndarray:\n",
    "    \n",
    "        display_frame = numpy.copy(source_image)\n",
    "        \n",
    "        # draw blue circles onto each joint \n",
    "        class_.hand_drawer.draw_landmarks(\n",
    "            display_frame, hand_landmarks, class_.hand_detector.HAND_CONNECTIONS,\n",
    "            class_.hand_drawer.DrawingSpec(color = (255, 0, 0), thickness = 2, circle_radius = 5))\n",
    "        \n",
    "        return display_frame\n",
    "    \n",
    "    @staticmethod\n",
    "    def finger_up(M, U, Z) -> bool:\n",
    "        \n",
    "        # if the upper joint is closer to the palm than the lower joint, it means the finger is bent\n",
    "        # if the finger is not bent, it is pointing up\n",
    "        return (math.dist(M, Z) >= math.dist(U, Z))\n",
    "\n",
    "    @classmethod\n",
    "    def is_rock(class_, points) -> bool:\n",
    "        \n",
    "        # all fingers excluding the thumb is pointing down\n",
    "        return not any([\n",
    "            class_.finger_up(points[ 7], points[ 6], points[0]),\n",
    "            class_.finger_up(points[11], points[10], points[0]),\n",
    "            class_.finger_up(points[15], points[14], points[0]),\n",
    "            class_.finger_up(points[19], points[18], points[0])\n",
    "        ])\n",
    "\n",
    "    @classmethod\n",
    "    def is_paper(class_, points) -> bool:\n",
    "        \n",
    "        # all fingers excluding the thumb is pointing up\n",
    "        return all([\n",
    "            class_.finger_up(points[ 7], points[ 6], points[0]),\n",
    "            class_.finger_up(points[11], points[10], points[0]),\n",
    "            class_.finger_up(points[15], points[14], points[0]),\n",
    "            class_.finger_up(points[19], points[18], points[0])\n",
    "        ])\n",
    "\n",
    "    @classmethod\n",
    "    def is_scissor(class_, points) -> bool:\n",
    "        \n",
    "        # index and middle finger pointing up but ring and pinky finger are pointing down\n",
    "        return all([\n",
    "            class_.finger_up(points[ 7], points[ 6], points[0]),\n",
    "            class_.finger_up(points[11], points[10], points[0])\n",
    "        ]) and not any([\n",
    "            class_.finger_up(points[15], points[14], points[0]),\n",
    "            class_.finger_up(points[19], points[18], points[0])\n",
    "        ])\n",
    "\n",
    "    @classmethod\n",
    "    def get_hand_posture(class_, points) -> int:\n",
    "        if   (class_.is_paper(  points)):\n",
    "            return 1\n",
    "        elif (class_.is_rock(   points)):\n",
    "            return 2\n",
    "        elif (class_.is_scissor(points)):\n",
    "            return 3\n",
    "        else:\n",
    "            # if an undetermined combination is detected, \"unknown\" will be the result\n",
    "            return 0\n",
    "\n",
    "    @staticmethod\n",
    "    def posture_to_text(posture) -> str:\n",
    "        \n",
    "        # obtain the text to display on the output frame\n",
    "        postures = [ \"Unknown\", \"Paper\", \"Rock\", \"Scissor\" ]\n",
    "        \n",
    "        return postures[posture]\n",
    "\n",
    "class CamReader:\n",
    "    \n",
    "    cam_stream = None\n",
    "    \n",
    "    @classmethod\n",
    "    def initialize(class_, camera : int = 0) -> \"CamReader\":\n",
    "        class_.cam_stream = cv2.VideoCapture(camera)\n",
    "        if not (class_.cam_stream.isOpened()):\n",
    "            raise IOError(\"Cannot open video camera {}\\n\".format(camera))\n",
    "        return class_\n",
    "    \n",
    "    @classmethod\n",
    "    def read(class_) -> Iterator[ numpy.ndarray ]:\n",
    "        while True:\n",
    "            yield class_.cam_stream.read()[1]\n",
    "            \n",
    "    @classmethod\n",
    "    def find_shape(class_) -> Dict[ str, int ]:\n",
    "        return {\n",
    "            \"HEIGHT\" : class_.cam_stream.get(cv2.CAP_PROP_FRAME_HEIGHT),\n",
    "            \"WIDTH\"  : class_.cam_stream.get(cv2.CAP_PROP_FRAME_WIDTH)\n",
    "        }\n",
    "            \n",
    "    @classmethod\n",
    "    def release(class_) -> None:\n",
    "        class_.cam_stream.release()\n",
    "        class_.cam_stream = None\n",
    "    \n",
    "if (__name__ == \"__main__\"):\n",
    "    \n",
    "    camera_number            = 0\n",
    "    \n",
    "    min_detection_confidence = 0.7\n",
    "    \n",
    "    min_tracking_confidence  = 0.7\n",
    "    \n",
    "    window_name              = \"Rock Paper Scissors\"\n",
    "\n",
    "    # initialize and open the webcam labeled 0\n",
    "    CamReader.initialize(camera_number)\n",
    "        \n",
    "    # obtain the height and width of the camera\n",
    "    (height, width) = CamReader.find_shape().values()\n",
    "    \n",
    "    # initialize the mediapipe hand detector and configure the minimal confidence values\n",
    "    HandDetector.initialize().configure(min_detection_confidence, min_tracking_confidence)\n",
    "    \n",
    "    # endlessly read and horizontally flip frames from the webcam until Esc is pressed\n",
    "    for frame in (  cv2.flip(__frame, 1) for __frame in CamReader.read()  ):\n",
    "        \n",
    "        # detect hands using the mediapipe hand detector\n",
    "        detected_hands = HandDetector.detect(frame)\n",
    "        \n",
    "        # only consider cases where hands are detected\n",
    "        if (detected_hands.multi_hand_landmarks):\n",
    "            \n",
    "            # iterate through each detected hand\n",
    "            for hand_landmarks in detected_hands.multi_hand_landmarks:\n",
    "                \n",
    "                # draw the 21 finger joints (points) onto the output frame\n",
    "                frame = HandDetector.draw_landmarks(frame, hand_landmarks)\n",
    "        \n",
    "                # initialize an empty list to store the (x,y,z) coordinates of each joint\n",
    "                points = []\n",
    "            \n",
    "                # iterate through the 21 joints\n",
    "                for point in HandDetector.hand_detector.HandLandmark:\n",
    "                    \n",
    "                    # obtain (x, y, z) coordinates\n",
    "                    normalized_landmark = hand_landmarks.landmark[point]\n",
    "                    \n",
    "                    # obtain (x, y) coordinates\n",
    "                    pixel_coordinates_landmark = HandDetector.hand_drawer._normalized_to_pixel_coordinates(\n",
    "                        normalized_landmark.x, normalized_landmark.y, width, height)\n",
    "                    \n",
    "                    # add to list\n",
    "                    points.append((normalized_landmark.x, normalized_landmark.y, normalized_landmark.z))\n",
    "                    \n",
    "            # detect the hand posture and put text to the upper left corner of the output frame\n",
    "            frame = cv2.putText(frame, HandDetector.posture_to_text(HandDetector.get_hand_posture(points)), \n",
    "                (30, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 255), 1, cv2.LINE_AA)\n",
    "    \n",
    "        # display the output frame \n",
    "        cv2.imshow(window_name, frame)\n",
    "        \n",
    "        # terminate the program if Esc is pressed\n",
    "        if (cv2.waitKey(1) == 27):\n",
    "            break\n",
    "                        \n",
    "    cv2.destroyWindow(window_name)\n",
    "    \n",
    "    CamReader.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Problem 6</b>\n",
    "\n",
    "I believe myself to have finished all five problems. The final exam is more challenging compared to the mid-term exam. I'd been working on the second problem for 4 days because it was impratical to detect pedestrians on each frame using the HOG detector. To tackle this, I had to review the mid-term exam and the previous practices, and then revert to using background subtraction, thresholding and morphology to improve the performance. Nevertheless, I have learned a lot from this course. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Problem 7</b>\n",
    "\n",
    "The overall execution of this class is excellent beyond words. To further improve this course, I believe that compressing the materials for each practice into separate zip files would make downloading easier. There had been a handful of times where the professor had already started explaining the code but Google Drive could not load everything because of bad Internet. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
